# -*- coding: utf-8 -*-
"""hw4_group38_ssitoot_kvinayak_kartkuma

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bTCFDibEPpNE3s8WqrTwf5uUTHy2lSLO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import tree
import collections

data = {
    "age": [24,53,23,25,32,52,22,43,52,48],
    "salary": [40000,52000,25000,77000,48000,110000,38000,44000,27000,65000],
    "Degree": [1,0,0,1,1,1,1,0,0,1]
}

df = pd.DataFrame(data)

class TreeNode():
    def __init__(self, attribute_idx=None, threshold=None, left=None, right=None, info_gain=None, value=None):
        # leaf node
        self.value = value 
        # decision node
        self.attribute_idx = attribute_idx
        self.threshold = threshold
        self.left = left
        self.right = right
        self.info_gain = info_gain

class DTree_model():
    def __init__(self, min_split=2, max_depth=2):
        self.root = None
        self.min_split = min_split
        self.max_depth = max_depth
    # Function to find entropy (Label)
    def entropy_deg(self, y):
        label = np.unique(y)
        entropy_deg = 0
        for i in label:
            #probability
            prob_i = len(y[y == i]) / len(y)
            entropy_deg =entropy_deg+ (-prob_i * np.log2(prob_i))
        return entropy_deg

    #function to find information gain
    def info_gain(self, parent, left_ch, right_ch): 
        left_wt = len(left_ch) / len(parent)
        right_wt = len(right_ch) / len(parent)
        #entropy of attributes
        entropy_att= left_wt*self.entropy_deg(left_ch) + right_wt*self.entropy_deg(right_ch)
        #information gain
        info_gain = self.entropy_deg(parent) - entropy_att
        return info_gain 
        
    def divide(self, df, attribute_idx, threshold):
      # this function is used to create splits in the root node based on the left and right nodes of 
      # threshold values
      df_left = []
      for r in df:
        if r[attribute_idx]<=threshold:
          df_left.append(r)
      df_left = np.array(df_left)
      df_right = []
      for r in df:
        if r[attribute_idx]>threshold:
          df_right.append(r)
      df_right = np.array(df_right)
      return df_left, df_right

    def best_split(self, df, total_sm, attributes):
        # this function is used for getting the best splits among all possible splits 
        # that is used by taking the threshold where information gain is the maximum
        current_split = {}
        maximum_gain = -float("inf")
        # loop through the thresholds of the attributes

        for attribute_idx in range(attributes):
            attribute_values = df[:, attribute_idx]
            thresholds_pred = np.unique(attribute_values)

            for threshold in thresholds_pred:

                df_left, df_right = self.divide(df, attribute_idx, threshold)

                if len(df_left)>0 and len(df_right)>0:
                    y= df[:, -1]
                    left_y= df_left[:, -1]
                    right_y= df_right[:, -1]

                    curr_info_gain = self.info_gain(y, left_y, right_y)

                    if curr_info_gain>maximum_gain:
                        current_split["df_left"] = df_left
                        current_split["df_right"] = df_right
                        current_split["attribute_idx"] = attribute_idx
                        current_split["info_gain"] = curr_info_gain
                        current_split["threshold"] = threshold
                        current_split["df_left"] = df_left
                        current_split["df_right"] = df_right
                        
                        maximum_gain = curr_info_gain
                      
        return current_split
    #Builiding decision tree    
    def build_tree(self, df, curr_depth=0):  
      #this function recursively builds the tree once the root is selected
        X= df[:,:-1]
        y= df[:,-1]
        total_sm= np.shape(X)[0] 
        attributes = np.shape(X)[1]
        if total_sm>=self.min_split and curr_depth<=self.max_depth:
            best_split = self.best_split(df, total_sm, attributes)
            if best_split["info_gain"]>0:
                left_subtree = self.build_tree(best_split["df_left"], curr_depth+1)
                right_subtree = self.build_tree(best_split["df_right"], curr_depth+1)
                return TreeNode(best_split["attribute_idx"], best_split["threshold"], 
                            left_subtree, right_subtree, best_split["info_gain"])
        val_leaf = self.calculate_val_leaf(y)
        node= TreeNode(value=val_leaf)
        return node
    
    def calculate_val_leaf(self, Y):
      #this function converts the y variable into a list and then takes the maximum
      #value based on its frequency
        
        Y = list(Y)
        return max(Y, key=Y.count)
    
    def print_tree(self, tree=None, indent=" "):
      #this function is used to recursively print the branches of the tree
        if not tree:
            tree = self.root

        if tree.value is not None:
            val= tree.value
            print(val)

        else:
            print("Attribute: "+str(tree.attribute_idx), "<=", tree.threshold)
            print('Amount of information at this node: {}'.format(f'{tree.info_gain:.3}'))
            print("%sleft:" % (indent), end="")
            self.print_tree(tree.left, indent + indent)
            print("%sright:" % (indent), end="")
            self.print_tree(tree.right, indent + indent)
    
    def fit(self, X, Y):
        
        df = np.concatenate((X, Y), axis=1)
        self.root = self.build_tree(df)
    
    def predict(self, X):
        for i in X:
          predictions= self.do_predict(i, self.root)
        return predictions
    
    def do_predict(self, x, tree):
        #this function recursively calls the predict function to move between the nodes of the
        if tree.value!=None: 
          return tree.value
        attribute_val = x[tree.attribute_idx]
        if attribute_val<=tree.threshold:
            return self.do_predict(x, tree.left)
        else:
            return self.do_predict(x, tree.right)

X_new = df.iloc[:,:-1].values
y_new = df.iloc[:,-1].values.reshape(-1,1)

X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=.2, random_state=41)

model = DTree_model(min_split=3, max_depth=4)
model.fit(X_train,y_train)
model.print_tree()

"""Univariate Decision Tree:
Advantages:
Univariate: Easy to understand, Faster model.

Multivariate:
Univariate involves only one variable whereas "multivariate" refers to the simultaneous use of several variates or variable quantities. Multivariate decision trees can therefore branch using all qualities at a single node.

Disadvantages:

Univariate: Can overfit easily.

Multivariate:
In multivariate trees, hyperplanes of any orientation are employed. 
This makes the use of search ineffective and unworkable. Therefore, utilizing a linear multivariate node that adds the weights for each feature is a more useful method to use. In order to make the procedure more effective and practical, linear multivariate decision trees select the most crucial features out of all of the available ones.

"""



"""References:
1. TinaGongting. “Implementing Decision Tree from Scratch in Python.” Medium, Medium, 6 June 2019, https://medium.com/@penggongting/implementing-decision-tree-from-scratch-in-python-c732e7c69aea. 

2. Bujokas, Eligijus. “Decision Tree Algorithm in Python from Scratch.” Medium, Towards Data Science, 14 Apr. 2021, https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173.

3. guest_blog. “Decision Tree Implementation in Python from Scratch.” Analytics Vidhya, 22 June 2022, https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/. 

4. Valkov, Venelin. “Building a Decision Tree from Scratch in Python.” Medium, Level Up Coding, 5 July 2019, https://levelup.gitconnected.com/building-a-decision-tree-from-scratch-in-python-machine-learning-from-scratch-part-ii-6e2e56265b19. 
"""

